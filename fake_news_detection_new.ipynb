{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras import regularizers, initializers, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5200, 4), (20800, 5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.read_csv(r\"C:\\Users\\THAKUR ABHINAV\\Desktop\\test.csv\")\n",
    "train=pd.read_csv(r\"C:\\Users\\THAKUR ABHINAV\\Desktop\\train.csv\")\n",
    "test.shape,train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20800,), (20800,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=train[\"title\"]+\" \"+train[\"text\"]\n",
    "y_train=train.label\n",
    "x_train.shape,y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(y_train)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words(\"english\")\n",
    "len(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    output=\"\"\n",
    "    text=str(text).replace(r'http[\\w:/\\.]+','') # removing urls\n",
    "    text=str(text).replace(r'[^\\.\\w\\s]','') #remove everything but characters and punctuation \n",
    "    text=str(text).replace(r'\\.\\.+',' ') #replace multiple periods with a single one \n",
    "    text=str(text).replace(r'\\.',' . ') #replace periods with a single one \n",
    "    text=str(text).replace(r'\\s\\s+',' ') #replace multiple white space with a single one \n",
    "    text=str(text).replace(\"\\n\", \"\") #removing line breaks\n",
    "    text=re.sub(r'[^\\w\\s]','',text).lower() #to upper to lower\n",
    "    text=text.split(\" \")\n",
    "    for word in text:\n",
    "        if word not in stop:\n",
    "            output=output+\" \"+word\n",
    "    return str(output.strip())[0:].replace(\" \",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THAKUR ABHINAV\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2c2e17d7e746f69823982051a6e5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20800.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts=[]\n",
    "for line in tqdm_notebook(x_train,total=train.shape[0]):\n",
    "    texts.append(clean_text(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20800"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filename,text):\n",
    "    lines=list()\n",
    "    for l in text:\n",
    "        lines.append(l)\n",
    "    data=\"\\n\".join(lines)\n",
    "    file = open(filename, \"w+\", encoding=\"utf-8\")\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(\"ram.txt\",texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_file(filename):\n",
    "    file=open(filename,'r+',encoding=\"utf-8\")\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abc=load_txt_file(\"ram.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20800"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textz=abc.split('\\n')\n",
    "len(textz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test=test[\"title\"]+\" \"+test[\"text\"]\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5200, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THAKUR ABHINAV\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7575118490ce425086765d3e3f66a5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_test=[]\n",
    "for line in tqdm_notebook(x_test,total=test.shape[0]):\n",
    "    text_test.append(clean_text(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS=100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 226684\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Vocabulary size:', len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (20800, 1000)\n"
     ]
    }
   ],
   "source": [
    "#padding\n",
    "\n",
    "data = pad_sequences(sequences, padding = 'post', maxlen = 1000) #post means 0 at last\n",
    "print('Shape of data tensor:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1,x_val,y_train1,y_val=train_test_split(data,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16640, 1000), (16640, 2), (4160, 1000), (4160, 2))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train1.shape,y_train1.shape,x_val.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226684"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings\n",
    "\n",
    "embedding_index={}\n",
    "file=open(\"glove100.txt\",encoding=\"utf-8\")\n",
    "for line in file:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    embedding_index[word]=np.asarray(values[1:],dtype='float32')\n",
    "file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100 # embedding dimensions for word vectors\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word,i in word_index.items():\n",
    "    embedding_vector=embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226685, 100)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''input_x = Input(shape=(None,))\n",
    "x=Embedding(len(word_index) + 1,\n",
    "  EMBEDDING_DIM,\n",
    "  weights = [embedding_matrix],\n",
    "  input_length = 1000,\n",
    "  trainable=False,\n",
    "  name = 'embeddings')(input_x)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x1=Convolution1D(filters=128, kernel_size=3,activation='relu')(x)\n",
    "#x1_2=MaxPool1D(pool_size=5)(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x2=Convolution1D(filters=128, kernel_size=4,activation='relu')(x)\n",
    "#x2_2=MaxPool1D(pool_size=5)(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x3=Convolution1D(filters=128, kernel_size=5,activation='relu')(x)\n",
    "#x3_2=MaxPool1D(pool_size=5)(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x4=concatenate([x1_2,x2_2,x3_2],axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x5=Convolution1D(filters=128, kernel_size=5,activation='relu')(x4)\n",
    "#x5_2=MaxPool1D(pool_size=5)(x5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(128)])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x6=Convolution1D(filters=128, kernel_size=5,activation='relu')(x5_2)\n",
    "#x6_2=MaxPool1D(pool_size=5)(x6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None)])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x7=Flatten()(x6_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x8=Dense(128,activation='relu')(x7)\n",
    "#x8_2=Dropout(rate=0.2)(x8)\n",
    "MAX_SEQUENCE_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(Input(shape=(MAX_SEQUENCE_LENGTH), dtype='int32'))\n",
    "model.add(Embedding(len(word_index) + 1,EMBEDDING_DIM,weights = [embedding_matrix],input_length = MAX_SEQUENCE_LENGTH,trainable=False,\n",
    " name = 'embeddings',input_shape=(1000,)))\n",
    "model.add(LSTM(60, return_sequences=True,name='lstm_layer'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#with tf.device(\"/device:GPU:0\"):\n",
    " #   model.fit(x_train1, y_train1, epochs = 10, batch_size=128, validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
